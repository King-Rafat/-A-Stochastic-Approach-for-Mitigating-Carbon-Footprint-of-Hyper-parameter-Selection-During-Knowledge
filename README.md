# Official Repository for the paper accepted in PLOS ONE journal under the name Mitigating Carbon Footprint for Knowledge Distillation Based Deep Learning Model Compression STKD_CFMitigation

Github Repository for DATA-FREE KNOWLEDGE DISTILLATION: https://github.com/zju-vipa/CMI

Github Repository for OBJECT DETECTION KNOWLEDGE DISTILLATION: https://github.com/SsisyphusTao/Object-Detection-Knowledge-Distillation/tree/mbv2-lite

Datasets

CIFAR 10: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

CIFAR 100: https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz

Tiny ImageNet: http://cs231n.stanford.edu/tiny-imagenet-200.zip

Pascal voc 2012:  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar

Pascal voc 2007:  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar, and http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar


